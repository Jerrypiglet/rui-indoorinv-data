data = {
    dataset_root = data/indoor_synthetic
    xml_root = data/indoor_synthetic
    xml_file = test.xml # relative to xml_root
}

scene = {
    shape_file = "" # relative to dataset_root
    frame_id_list = []
    invalid_frame_id_list = []
}

scene_params_dict = {
    mitsuba_version = "3.0.0"
    axis_up = "y+"
    pose_file = "json-transforms.json" # requires scaled Blender scene! in comply with Liwen's IndoorDataset (https://github.com/william122742/inv-nerf/blob/bake/utils/dataset/indoor.py)
    # 'monosdf_shape_dict': monosdf_shape_dict, # comment out if load GT shape from XML; otherwise load shape from MonoSDF to **'shape' and Mitsuba scene**
    }

im_params_dict = {
    im_H_load = 320
    im_W_load = 640
    im_H_resize = 320
    im_W_resize = 640
    spp = 4096
    }

modality_filename_dict = {
    im_hdr = Image/%03d_0001.exr
    im_sdr = Image/%03d_0001.png
    albedo = DiffCol/%03d_0001.exr
    roughness = Roughness/%03d_0001.exr
    emission = Emit/%03d_0001.exr
    depth = Depth/%03d_0001.exr
    normal = Normal/%03d_0001.exr
    # 'lighting_envmap', 
    # 'layout', 
}

cam_params_dict = {
    near = 0.1
    far = 10.0
    sampleNum = 3
    
    # == params for sample camera poses
    heightMin = 0.5 # camera height min
    heightMax = 2.5 # camera height max
    distMin = 0.2 # to wall distance min
    distMax = 3 # to wall distance max
    thetaMin = -60 # theta min: pitch angle; up+ 
    thetaMax = 40 # theta max: pitch angle; up+
    phiMin = -60 # yaw angle min
    phiMax = 60 # yaw angle max
    distRaysMin = 0.2 # min dist of all camera rays to the scene; [!!!] set to -1 to disable checking
    distRaysMedianMin = 0.6 # median dist of all camera rays to the scene; [!!!] set to -1 to disable checking
}

lighting_params_dict = {
    SG_num = 12
    env_row = 8
    env_col = 16 # resolution to load; FIXED
    env_downsample_rate = 2 # (8, 16) -> (4, 8)

    # 'env_height': 2, 'env_width': 4, 
    # 'env_height': 8, 'env_width': 16, 
    # 'env_height': 128, 'env_width': 256, 
    env_height = 256
    env_width = 512
}

mi_params_dict = {
    debug_render_test_image = True # [DEBUG][slow] True: to render an image with first camera, usig Mitsuba: images/demo_mitsuba_render.png
    debug_dump_mesh = True # [DEBUG] True: to dump all object meshes to mitsuba/meshes_dump; load all .ply files into MeshLab to view the entire scene: images/demo_mitsuba_dump_meshes.png
    if_sample_rays_pts = True # True: to sample camera rays and intersection pts given input mesh and camera poses
    if_get_segs = True # [depend on if_sample_rays_pts] True: to generate segs similar to those in openroomsScene2D.load_seg()

}

shape_params_dict = {
    if_load_obj_mesh = True # set to False to not load meshes for objs (furniture) to save time
    if_load_emitter_mesh = True  # default True: to load emitter meshes, because not too many emitters

    if_sample_pts_on_mesh = False  # default True: sample points on each shape -> self.sample_pts_list
    sample_mesh_ratio = 0.1 # target num of VERTICES: len(vertices) * sample_mesh_ratio
    sample_mesh_min = 10
    sample_mesh_max = 100

    if_simplify_mesh = False # default True: simply triangles
    simplify_mesh_ratio = 0.1 # target num of FACES: len(faces) * simplify_mesh_ratio
    simplify_mesh_min = 100
    simplify_mesh_max = 1000
    if_remesh = True # False: images/demo_shapes_3D_kitchen_NO_remesh.png; True: images/demo_shapes_3D_kitchen_YES_remesh.png
    remesh_max_edge = 0.15
    
    if_dump_shape = False # True to dump fixed shape to obj file
    if_fix_watertight = False
}