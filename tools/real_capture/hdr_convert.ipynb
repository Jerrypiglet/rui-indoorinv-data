{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e048c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import Any, Dict, Optional\n",
    "from typing_extensions import Literal\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm \n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\"\n",
    "\n",
    "import rawpy\n",
    "import lensfunpy\n",
    "\n",
    "from funcs import read_process_img, process_bracket, get_files\n",
    "\n",
    "# Conference Room\n",
    "root_dir = Path(\"../../data/real/ConferenceRoomV2_final_supergloo_tmp\")\n",
    "times = np.array([1/15, 1/30, 1/8, 1/60, 1/4, 1/125, 1/2, 1/250, 1], dtype=np.float32)\n",
    "METHOD = \"superglue\"\n",
    "calib_dir = root_dir.parent / \"Sony/calib_raw\"\n",
    "\n",
    "image_dir = root_dir / \"raw_images\"\n",
    "calib_paths = get_files(calib_dir)\n",
    "image_paths = get_files(image_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ceb2f612",
   "metadata": {},
   "source": [
    "# Get calibration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "937d88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 4\n",
    "p = image_paths[index]\n",
    "img_max = 2**14 - 1\n",
    "\n",
    "with rawpy.imread(str(p)) as raw:\n",
    "    raw_processed = cv2.demosaicing(raw.raw_image, code=cv2.COLOR_BAYER_BG2BGR)\n",
    "    \n",
    "height, width, _ = raw_processed.shape\n",
    "\n",
    "db = lensfunpy.Database()\n",
    "# Sony Setting\n",
    "cam = db.find_cameras(\"Sony\", \"ILCE-7M3\")[0]\n",
    "lens = db.find_lenses(cam, \"Sony\", \"FE 24-105mm\")[0]\n",
    "focal_length, aperture, distance = 24, 20, 0.5\n",
    "# Canon Setting\n",
    "# cam = db.find_cameras(\"Canon\", \"Canon EOS 5D Mark III\")[0]\n",
    "# lens = db.find_lenses(cam, \"Canon\", \"Canon EF 24-70mm f/2.8L II USM\")[0]\n",
    "# focal_length, aperture, distance = 24, 19, 2.2\n",
    "\n",
    "mod = lensfunpy.Modifier(lens, cam.crop_factor, width, height)\n",
    "mod.initialize(focal_length, aperture, distance, scale=1.0, pixel_format=np.float32)\n",
    "\n",
    "# Sony\n",
    "white_balance = np.array([2.6657157, 1.0 ,1.9722375], dtype=np.float32)\n",
    "# # Canon\n",
    "# white_balance = np.array([2.5430965, 1., 1.5503172], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57afc4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d4b4b5b216446dbb67fc0695975023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprojection error: 0.7250952927240312\n"
     ]
    }
   ],
   "source": [
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# downsample images, resolution is too high\n",
    "downsample_scale = 8\n",
    "checkerboard_size = (16, 13)\n",
    "# termination criteria\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 500, 0.001)\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((np.prod(checkerboard_size),3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:checkerboard_size[0],0:checkerboard_size[1]].T.reshape(-1,2)\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d point in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "# Main loop for extracting corners\n",
    "for calib_img_path in tqdm(calib_paths):\n",
    "    assert Path(calib_img_path).exists()\n",
    "    img = read_process_img(calib_img_path, mod=mod) * white_balance\n",
    "    # Linear to grayscale conversion with scaling\n",
    "    img_gray = img[...,2] * 0.2126 + img[...,1] * 0.7152 + img[...,0] * 0.0722\n",
    "#     img_gray = np.average(img, axis=-1)\n",
    "    img_gray /= np.max(img_gray)\n",
    "    img_gray_resized = cv2.resize(\n",
    "        img_gray, \n",
    "        dsize=(img.shape[1] // downsample_scale, img.shape[0] // downsample_scale), \n",
    "        interpolation=cv2.INTER_AREA)\n",
    "    ret, corners_resized = cv2.findChessboardCorners(\n",
    "        (img_gray_resized * 255).astype(np.uint8), checkerboard_size, None)\n",
    "    # If found, add object points, image points (after refining them)\n",
    "    if ret == True:\n",
    "        corners = corners_resized * downsample_scale\n",
    "        objpoints.append(objp)\n",
    "        corners2 = cv2.cornerSubPix(img_gray, np.floor(corners), (40,40), (-1,-1), criteria)\n",
    "        imgpoints.append(corners2)\n",
    "        \n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_gray.shape[::-1], None, None)\n",
    "print(\"Reprojection error:\", ret)\n",
    "\n",
    "# fig = plt.figure(figsize=(8,6))\n",
    "# plt.imshow(img_gray)\n",
    "# plt.plot(corners2[:,0,0], corners2[:,0,1], \"ro\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d209626",
   "metadata": {},
   "source": [
    "# Prepare folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfd2e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = img_gray.shape[:2]\n",
    "exr_resize_shape = (1152, 768)\n",
    "upsample_factor = int(np.max(np.ceil(img_gray.shape[-1::-1] / np.array(exr_resize_shape))))\n",
    "\n",
    "w_new, h_new = np.array(exr_resize_shape) * upsample_factor\n",
    "newcameramtx, roi = cv2.getOptimalNewCameraMatrix(\n",
    "    mtx, dist, (w,h), \n",
    "    0, # Contain only the valid pixels\n",
    "    (w_new,h_new)\n",
    ")\n",
    "mapx, mapy = cv2.initUndistortRectifyMap(mtx, dist, None, newcameramtx, (w_new,h_new), cv2.CV_32F)\n",
    "\n",
    "# Create directories\n",
    "exr_output_dir = root_dir / \"merged_images\"\n",
    "exr_output_dir.mkdir(exist_ok=True)\n",
    "png_output_dir = root_dir / \"png_images\"\n",
    "png_output_dir.mkdir(exist_ok=True)\n",
    "pose_output_dir = root_dir / \"pose_images\"\n",
    "pose_output_dir.mkdir(exist_ok=True)\n",
    "# Setup the image scales and output brightness\n",
    "pose_resize_scale = 4\n",
    "png_exposure_scale = 1.5\n",
    "depth_normal_exposure_scale = 2.0\n",
    "# Setup the params for combining images\n",
    "apply_median_blur = False\n",
    "times_min_index = np.argmin(times)\n",
    "times_max_index = np.argmax(times)\n",
    "exposure_target = times[-1]\n",
    "# Depth Normal estimation\n",
    "estimate_depth = True\n",
    "estimate_normals = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d9cfc",
   "metadata": {},
   "source": [
    "# Process RAW images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e00b2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d8148376d146f5b00207d61f57cba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/285 [00:08<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "self.lf cannot be converted to a Python object for pickling",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jerrypiglet/Documents/Projects/OpenRooms_RAW_loader/tools/real_capture/hdr_convert.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrypiglet/Documents/Projects/OpenRooms_RAW_loader/tools/real_capture/hdr_convert.ipynb#Y115sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m _list \u001b[39m=\u001b[39m [(index, params) \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m index_list]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrypiglet/Documents/Projects/OpenRooms_RAW_loader/tools/real_capture/hdr_convert.ipynb#Y115sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(_list), \u001b[39mlen\u001b[39m(index_list))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jerrypiglet/Documents/Projects/OpenRooms_RAW_loader/tools/real_capture/hdr_convert.ipynb#Y115sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mlist\u001b[39;49m(tqdm(p\u001b[39m.\u001b[39;49mimap_unordered(process_bracket, _list), total\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(index_list)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrypiglet/Documents/Projects/OpenRooms_RAW_loader/tools/real_capture/hdr_convert.ipynb#Y115sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m p\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jerrypiglet/Documents/Projects/OpenRooms_RAW_loader/tools/real_capture/hdr_convert.ipynb#Y115sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m p\u001b[39m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/miniconda3/envs/or-py310/lib/python3.10/site-packages/tqdm/notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 259\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    260\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    262\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/or-py310/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/or-py310/lib/python3.10/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/miniconda3/envs/or-py310/lib/python3.10/multiprocessing/pool.py:540\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     put(task)\n\u001b[1;32m    541\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m     job, idx \u001b[39m=\u001b[39m task[:\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/or-py310/lib/python3.10/multiprocessing/connection.py:211\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_writable()\n\u001b[0;32m--> 211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_bytes(_ForkingPickler\u001b[39m.\u001b[39;49mdumps(obj))\n",
      "File \u001b[0;32m~/miniconda3/envs/or-py310/lib/python3.10/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdumps\u001b[39m(\u001b[39mcls\u001b[39m, obj, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mcls\u001b[39;49m(buf, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m buf\u001b[39m.\u001b[39mgetbuffer()\n",
      "File \u001b[0;32mstringsource:2\u001b[0m, in \u001b[0;36mlensfunpy._lensfun.Modifier.__reduce_cython__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: self.lf cannot be converted to a Python object for pickling"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "import copy\n",
    "\n",
    "params = {\n",
    "    'exr_output_dir': exr_output_dir, \n",
    "    'pose_output_dir': pose_output_dir, \n",
    "    'png_output_dir': png_output_dir, \n",
    "    'times': times, \n",
    "    'image_paths': image_paths, \n",
    "    'times_max_index': times_max_index, \n",
    "    'times_min_index': times_min_index, \n",
    "    'exposure_target': exposure_target, \n",
    "    'white_balance': white_balance, \n",
    "    'apply_median_blur': apply_median_blur, \n",
    "    'mapx': mapx, \n",
    "    'mapy': mapy, \n",
    "    'exr_resize_shape': exr_resize_shape, \n",
    "    'png_exposure_scale': png_exposure_scale, \n",
    "    'pose_resize_scale': pose_resize_scale, \n",
    "    'mod': mod,\n",
    "    # 'height': height, \n",
    "    # 'width': width,\n",
    "}\n",
    "\n",
    "tic = time.time()\n",
    "p = Pool(processes=1)\n",
    "index_list = range(len(image_paths)//len(times))\n",
    "_list = [(index, params) for index in index_list]\n",
    "print(len(_list), len(index_list))\n",
    "list(tqdm(p.imap_unordered(process_bracket, _list), total=len(index_list)))\n",
    "p.close()\n",
    "p.join()\n",
    "print('==== ...DONE. Took %.2f seconds'%(time.time() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa566fec",
   "metadata": {},
   "source": [
    "## Colmap reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bcead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfstudio.process_data.colmap_utils import get_colmap_version, get_vocab_tree\n",
    "from nerfstudio.utils.rich_utils import status\n",
    "from nerfstudio.utils.scripts import run_command\n",
    "from rich.console import Console\n",
    "CONSOLE = Console(width=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ff61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CameraModel(Enum):\n",
    "    \"\"\"Enum for camera types.\"\"\"\n",
    "\n",
    "    OPENCV = \"OPENCV\"\n",
    "    OPENCV_FISHEYE = \"OPENCV_FISHEYE\"\n",
    "    PINHOLE = \"PINHOLE\"\n",
    "    SIMPLE_PINHOLE = \"SIMPLE_PINHOLE\"\n",
    "\n",
    "\n",
    "def run_colmap(\n",
    "    image_dir: Path,\n",
    "    colmap_dir: Path,\n",
    "    camera_model: CameraModel,\n",
    "    camera_mask_path: Optional[Path] = None,\n",
    "    camera_params = None,\n",
    "    gpu: bool = True,\n",
    "    verbose: bool = False,\n",
    "    matching_method: Literal[\"vocab_tree\", \"exhaustive\", \"sequential\"] = \"vocab_tree\",\n",
    "    colmap_cmd: str = \"colmap\",\n",
    "):\n",
    "    camera_params = '\"' + \",\".join([i.astype(str) for i in camera_params]) + '\"'\n",
    "    colmap_version = get_colmap_version(colmap_cmd)\n",
    "\n",
    "    colmap_database_path = colmap_dir / \"database.db\"\n",
    "    if colmap_database_path.exists():\n",
    "        # Can't use missing_ok argument because of Python 3.7 compatibility.\n",
    "        colmap_database_path.unlink()\n",
    "\n",
    "    # Feature extraction\n",
    "    feature_extractor_cmd = [\n",
    "        f\"{colmap_cmd} feature_extractor\",\n",
    "        f\"--database_path {colmap_dir / 'database.db'}\",\n",
    "        f\"--image_path {image_dir}\",\n",
    "        \"--ImageReader.single_camera 1\",\n",
    "        f\"--ImageReader.camera_model {camera_model.value}\",\n",
    "        f\"--ImageReader.camera_params {camera_params}\",\n",
    "        f\"--SiftExtraction.use_gpu {int(gpu)}\",\n",
    "    ]\n",
    "    if camera_mask_path is not None:\n",
    "        feature_extractor_cmd.append(f\"--ImageReader.camera_mask_path {camera_mask_path}\")\n",
    "    feature_extractor_cmd = \" \".join(feature_extractor_cmd)\n",
    "    with status(msg=\"[bold yellow]Running COLMAP feature extractor...\", spinner=\"moon\", verbose=verbose):\n",
    "        run_command(feature_extractor_cmd, verbose=verbose)\n",
    "\n",
    "    CONSOLE.log(\"[bold green]:tada: Done extracting COLMAP features.\")\n",
    "\n",
    "    # Feature matching\n",
    "    feature_matcher_cmd = [\n",
    "        f\"{colmap_cmd} {matching_method}_matcher\",\n",
    "        f\"--database_path {colmap_dir / 'database.db'}\",\n",
    "        f\"--SiftMatching.use_gpu {int(gpu)}\",\n",
    "    ]\n",
    "    if matching_method == \"vocab_tree\":\n",
    "        vocab_tree_filename = get_vocab_tree()\n",
    "        feature_matcher_cmd.append(f\"--VocabTreeMatching.vocab_tree_path {vocab_tree_filename}\")\n",
    "    feature_matcher_cmd = \" \".join(feature_matcher_cmd)\n",
    "    with status(msg=\"[bold yellow]Running COLMAP feature matcher...\", spinner=\"runner\", verbose=verbose):\n",
    "        run_command(feature_matcher_cmd, verbose=verbose)\n",
    "    CONSOLE.log(\"[bold green]:tada: Done matching COLMAP features.\")\n",
    "\n",
    "    # Bundle adjustment\n",
    "    sparse_dir = colmap_dir / \"sparse\"\n",
    "    sparse_dir.mkdir(parents=True, exist_ok=True)\n",
    "    mapper_cmd = [\n",
    "        f\"{colmap_cmd} mapper\",\n",
    "        f\"--database_path {colmap_dir / 'database.db'}\",\n",
    "        f\"--image_path {image_dir}\",\n",
    "        f\"--output_path {sparse_dir}\",\n",
    "        \"--Mapper.ba_refine_focal_length 0\",\n",
    "        \"--Mapper.ba_refine_extra_params 0\",\n",
    "    ]\n",
    "    if colmap_version >= 3.7:\n",
    "        mapper_cmd.append(\"--Mapper.ba_global_function_tolerance 1e-6\")\n",
    "\n",
    "    mapper_cmd = \" \".join(mapper_cmd)\n",
    "\n",
    "    with status(\n",
    "        msg=\"[bold yellow]Running COLMAP bundle adjustment... (This may take a while)\",\n",
    "        spinner=\"circle\",\n",
    "        verbose=verbose,\n",
    "    ):\n",
    "        run_command(mapper_cmd, verbose=verbose)\n",
    "    CONSOLE.log(\"[bold green]:tada: Done COLMAP bundle adjustment.\")\n",
    "    with status(msg=\"[bold yellow]Refine intrinsics...\", spinner=\"dqpb\", verbose=verbose):\n",
    "        bundle_adjuster_cmd = [\n",
    "            f\"{colmap_cmd} bundle_adjuster\",\n",
    "            f\"--input_path {sparse_dir}/0\",\n",
    "            f\"--output_path {sparse_dir}/0\",\n",
    "            \"--BundleAdjustment.refine_principal_point 0\",\n",
    "            \"--BundleAdjustment.refine_focal_length 0\",\n",
    "            \"--BundleAdjustment.refine_extra_params 0\",\n",
    "        ]\n",
    "        run_command(\" \".join(bundle_adjuster_cmd), verbose=verbose)\n",
    "    CONSOLE.log(\"[bold green]:tada: Done refining intrinsics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e3022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hloc import (\n",
    "    extract_features,\n",
    "    match_features,\n",
    "    pairs_from_exhaustive,\n",
    "    pairs_from_retrieval,\n",
    "    reconstruction,\n",
    ")\n",
    "import pycolmap\n",
    "\n",
    "def run_hloc(\n",
    "    image_dir: Path,\n",
    "    colmap_dir: Path,\n",
    "    camera_model: CameraModel,\n",
    "    camera_params = None,\n",
    "    verbose: bool = False,\n",
    "    matching_method: Literal[\"vocab_tree\", \"exhaustive\", \"sequential\"] = \"vocab_tree\",\n",
    "    feature_type: Literal[\n",
    "        \"sift\", \"superpoint_aachen\", \"superpoint_max\", \"superpoint_inloc\", \"r2d2\", \"d2net-ss\", \"sosnet\", \"disk\"\n",
    "    ] = \"superpoint_aachen\",\n",
    "    matcher_type: Literal[\n",
    "        \"superglue\", \"superglue-fast\", \"NN-superpoint\", \"NN-ratio\", \"NN-mutual\", \"adalam\"\n",
    "    ] = \"superglue\",\n",
    "    num_matched: int = 50,\n",
    ") -> None:\n",
    "    camera_params = \",\".join([i.astype(str) for i in camera_params])\n",
    "    \n",
    "    outputs = colmap_dir\n",
    "    sfm_pairs = outputs / \"pairs-netvlad.txt\"\n",
    "    sfm_dir = outputs / \"sparse\" / \"0\"\n",
    "    features = outputs / \"features.h5\"\n",
    "    matches = outputs / \"matches.h5\"\n",
    "\n",
    "    retrieval_conf = extract_features.confs[\"netvlad\"]\n",
    "    feature_conf = extract_features.confs[feature_type]\n",
    "    matcher_conf = match_features.confs[matcher_type]\n",
    "\n",
    "    references = [p.relative_to(image_dir).as_posix() for p in image_dir.iterdir()]\n",
    "    extract_features.main(feature_conf, image_dir, image_list=references, feature_path=features)\n",
    "    if matching_method == \"exhaustive\":\n",
    "        pairs_from_exhaustive.main(sfm_pairs, image_list=references)\n",
    "    else:\n",
    "        retrieval_path = extract_features.main(retrieval_conf, image_dir, outputs)\n",
    "        if num_matched >= len(references):\n",
    "            num_matched = len(references)\n",
    "        pairs_from_retrieval.main(retrieval_path, sfm_pairs, num_matched=num_matched)\n",
    "    match_features.main(matcher_conf, sfm_pairs, features=features, matches=matches)\n",
    "\n",
    "    image_options = pycolmap.ImageReaderOptions(  # pylint: disable=c-extension-no-member\n",
    "        camera_model=camera_model.value,\n",
    "        camera_params=camera_params\n",
    "    )\n",
    "    min_match_score = None\n",
    "    skip_geometric_verification = False\n",
    "    \n",
    "    sfm_dir.mkdir(parents=True, exist_ok=True)\n",
    "    database = sfm_dir / 'database.db'\n",
    "    \n",
    "    reconstruction.create_empty_db(database)\n",
    "    reconstruction.import_images(\n",
    "        image_dir, database, pycolmap.CameraMode.SINGLE, references, image_options)\n",
    "    image_ids = reconstruction.get_image_ids(database)\n",
    "    reconstruction.import_features(image_ids, database, features)\n",
    "    reconstruction.import_matches(image_ids, database, sfm_pairs, matches,\n",
    "                   min_match_score, skip_geometric_verification)\n",
    "    \n",
    "    mapper_options = {\n",
    "      \"ba_refine_focal_length\": False,\n",
    "      \"ba_refine_principal_point\": False,\n",
    "      \"ba_refine_extra_params\": False,\n",
    "    }\n",
    "    \n",
    "    if not skip_geometric_verification:\n",
    "        reconstruction.estimation_and_geometric_verification(\n",
    "            database, sfm_pairs, verbose)\n",
    "    rec_out = reconstruction.run_reconstruction(\n",
    "        sfm_dir, database, image_dir, verbose, mapper_options)\n",
    "    if rec_out is not None:\n",
    "        CONSOLE.log(f'Reconstruction statistics:\\n{rec_out.summary()}'\n",
    "                    + f'\\n\\tnum_input_images = {len(image_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfbcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "colmap_dir = root_dir / \"reconstructions\" / \"reconstruction_colmap\"\n",
    "# colmap_dir = root_dir / \"reconstructions\" / \"reconstruction_hloc\"\n",
    "recon_dir = colmap_dir / \"sparse\" / \"0\"\n",
    "if colmap_dir.exists():\n",
    "    shutil.rmtree(colmap_dir)\n",
    "os.makedirs(colmap_dir)\n",
    "\n",
    "camera_params = np.array(\n",
    "    [newcameramtx[0,0], newcameramtx[1,1], newcameramtx[0, 2], newcameramtx[1, 2]])\n",
    "camera_params *= pose_resize_scale / upsample_factor\n",
    "\n",
    "camera_model = CameraModel.PINHOLE\n",
    "\n",
    "if METHOD.lower() == 'colmap':\n",
    "    # USE COLMAP\n",
    "    run_colmap(\n",
    "        image_dir=pose_output_dir,\n",
    "        colmap_dir=colmap_dir,\n",
    "        camera_model=camera_model,\n",
    "        camera_params=camera_params,\n",
    "        gpu=True,\n",
    "        verbose=True,\n",
    "        matching_method=\"exhaustive\"\n",
    "    ); appendix = '_colmap'\n",
    "\n",
    "elif METHOD.lower() == 'superglue':\n",
    "    # USE Superglue\n",
    "    run_hloc(\n",
    "        image_dir=pose_output_dir,\n",
    "        colmap_dir=colmap_dir,\n",
    "        camera_model=camera_model,\n",
    "        camera_params=camera_params,\n",
    "        verbose=True,\n",
    "        matching_method=\"exhaustive\",\n",
    "        feature_type=\"superpoint_inloc\",\n",
    "        matcher_type=\"superglue\",\n",
    "    ); appendix = '_supergloo'\n",
    "else:\n",
    "    raise ValueError(f\"Unknown method {METHOD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0692ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfstudio.utils.io import load_from_json, write_to_json\n",
    "from nerfstudio.data.utils.colmap_parsing_utils import (\n",
    "    read_cameras_binary, read_images_binary, qvec2rotmat, \n",
    ")\n",
    "    \n",
    "from nerfstudio.process_data.colmap_utils import parse_colmap_camera_params\n",
    "\n",
    "recon_dir = colmap_dir / \"sparse\" / \"0\"\n",
    "\n",
    "cam_id_to_camera = read_cameras_binary(recon_dir / \"cameras.bin\")\n",
    "im_id_to_image = read_images_binary(recon_dir / \"images.bin\")\n",
    "\n",
    "print(f\"{len(im_id_to_image)} frames matched..\")\n",
    "\n",
    "frames = []\n",
    "for im_id, im_data in im_id_to_image.items():\n",
    "    # NB: COLMAP uses Eigen / scalar-first quaternions\n",
    "    # * https://colmap.github.io/format.html\n",
    "    # * https://github.com/colmap/colmap/blob/bf3e19140f491c3042bfd85b7192ef7d249808ec/src/base/pose.cc#L75\n",
    "    # the `rotation_matrix()` handles that format for us.\n",
    "\n",
    "    # TODO(1480) BEGIN use pycolmap API\n",
    "    # rotation = im_data.rotation_matrix()\n",
    "    rotation = qvec2rotmat(im_data.qvec)\n",
    "\n",
    "    translation = im_data.tvec.reshape(3, 1)\n",
    "    w2c = np.concatenate([rotation, translation], 1)\n",
    "    w2c = np.concatenate([w2c, np.array([[0, 0, 0, 1]])], 0)\n",
    "    c2w = np.linalg.inv(w2c)\n",
    "    # Convert from COLMAP's camera coordinate system (OpenCV) to ours (OpenGL)\n",
    "    c2w[0:3, 1:3] *= -1\n",
    "    c2w = c2w[np.array([1, 0, 2, 3]), :]\n",
    "    c2w[2, :] *= -1\n",
    "\n",
    "    name = im_data.name.split(\".\")[0]+\".exr\"\n",
    "    name = Path(f\"./{exr_output_dir.name}/{name}\")\n",
    "    \n",
    "#     name = im_data.name.split(\".\")[0]+\".png\"\n",
    "#     name = Path(f\"./{png_output_dir.name}/{name}\")\n",
    "\n",
    "    frame = {\n",
    "        \"file_path\": name.as_posix(),\n",
    "        \"transform_matrix\": c2w.tolist(),\n",
    "#         \"colmap_im_id\": im_id,\n",
    "    }\n",
    "    frames.append(frame)\n",
    "    \n",
    "# Read camera params and write frame information\n",
    "transforms_json = parse_colmap_camera_params(cam_id_to_camera[1])\n",
    "transforms_json[\"frames\"] = frames\n",
    "\n",
    "# Scale camera params\n",
    "cam_params = np.array([\n",
    "    transforms_json[\"w\"], transforms_json[\"h\"], \n",
    "    transforms_json[\"fl_x\"], transforms_json[\"fl_y\"], transforms_json[\"cx\"], transforms_json[\"cy\"]])\n",
    "cam_param_scaled = cam_params / pose_resize_scale\n",
    "\n",
    "transforms_json[\"w\"], transforms_json[\"h\"], \\\n",
    "    transforms_json[\"fl_x\"], transforms_json[\"fl_y\"],\\\n",
    "    transforms_json[\"cx\"], transforms_json[\"cy\"] = tuple(cam_param_scaled)\n",
    "\n",
    "write_to_json(root_dir/(\"transforms%s.json\"%appendix), transforms_json)\n",
    "print(f\"Wrote info to {root_dir/('transforms%s.json'%appendix)} file!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
