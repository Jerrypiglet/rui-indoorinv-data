{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e048c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "from typing import Any, Dict, Optional\n",
    "from typing_extensions import Literal\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm \n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\"\n",
    "\n",
    "import rawpy\n",
    "import lensfunpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f60c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    path = Path(path)\n",
    "    files = []\n",
    "    for p in sorted(os.listdir(path)):\n",
    "        p = path / p\n",
    "        if p.is_file():\n",
    "            files.append(p)\n",
    "    return files\n",
    "\n",
    "# Conference Room\n",
    "root_dir = Path(\"/Data/Datasets/IndoorInverseRendering/Sony/ConferenceRoomV2_final_supergloo\")\n",
    "times = np.array([1/15, 1/30, 1/8, 1/60, 1/4, 1/125, 1/2, 1/250, 1], dtype=np.float32)\n",
    "METHOD = \"superglue\"\n",
    "calib_dir = root_dir.parent / \"Sony/calib_raw\"\n",
    "\n",
    "image_dir = root_dir / \"raw_images\"\n",
    "calib_paths = get_files(calib_dir)\n",
    "image_paths = get_files(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 4\n",
    "p = image_paths[index]\n",
    "img_max = 2**14 - 1\n",
    "\n",
    "with rawpy.imread(str(p)) as raw:\n",
    "    raw_processed = cv2.demosaicing(raw.raw_image, code=cv2.COLOR_BAYER_BG2BGR)\n",
    "    \n",
    "height, width, _ = raw_processed.shape\n",
    "\n",
    "db = lensfunpy.Database()\n",
    "# Sony Setting\n",
    "cam = db.find_cameras(\"Sony\", \"ILCE-7M3\")[0]\n",
    "lens = db.find_lenses(cam, \"Sony\", \"FE 24-105mm\")[0]\n",
    "focal_length, aperture, distance = 24, 20, 0.5\n",
    "# Canon Setting\n",
    "# cam = db.find_cameras(\"Canon\", \"Canon EOS 5D Mark III\")[0]\n",
    "# lens = db.find_lenses(cam, \"Canon\", \"Canon EF 24-70mm f/2.8L II USM\")[0]\n",
    "# focal_length, aperture, distance = 24, 19, 2.2\n",
    "\n",
    "mod = lensfunpy.Modifier(lens, cam.crop_factor, width, height)\n",
    "mod.initialize(focal_length, aperture, distance, scale=1.0, pixel_format=np.float32)\n",
    "\n",
    "def read_process_img(\n",
    "    p,\n",
    "    mod=mod,\n",
    "#     crop_x=150,  # Canon Setup\n",
    "#     crop_y=100,\n",
    "    crop_x=100,  # Sony Setup\n",
    "    crop_y=50,\n",
    "    img_max=np.float32(2**14-1),\n",
    "):\n",
    "    # Images should be linear rgb for tiff(float) images\n",
    "    # https://lensfun.github.io/calibration-tutorial/lens-vignetting.html\n",
    "    with rawpy.imread(str(p)) as raw:\n",
    "        image = raw.raw_image.copy()\n",
    "        black = np.reshape(np.array(raw.black_level_per_channel, dtype=image.dtype), (2, 2))\n",
    "        black = np.tile(black, (image.shape[0]//2, image.shape[1]//2))\n",
    "        image = np.maximum(image, black) - black\n",
    "        image = cv2.demosaicing(image, code=cv2.COLOR_BAYER_BG2BGR)\n",
    "        image = image / (raw.white_level - black)[...,np.newaxis].astype(np.float32)\n",
    "    did_apply = mod.apply_color_modification(image)\n",
    "    image = np.clip(image, 0, 1)\n",
    "    return image[crop_y:-crop_y, crop_x:-crop_x]\n",
    "\n",
    "# img = read_process_img(image_paths[index])\n",
    "# wb_patch = img[2350:2400, 2490:2540]\n",
    "# fig = plt.figure(figsize=(4,4))\n",
    "# plt.imshow((wb_patch * 2.5)[...,[2,1,0]])\n",
    "# white_balance = np.mean(wb_patch, axis=(0,1))\n",
    "# white_balance = white_balance[1] / white_balance\n",
    "# Sony\n",
    "white_balance = np.array([2.6657157, 1.0 ,1.9722375], dtype=np.float32)\n",
    "# # Canon\n",
    "# white_balance = np.array([2.5430965, 1., 1.5503172], dtype=np.float32)\n",
    "# fig = plt.figure(figsize=(8,6))\n",
    "# plt.imshow((img * white_balance * 1.5)[...,[2,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57afc4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample images, resolution is too high\n",
    "downsample_scale = 8\n",
    "checkerboard_size = (16, 13)\n",
    "# termination criteria\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 500, 0.001)\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((np.prod(checkerboard_size),3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:checkerboard_size[0],0:checkerboard_size[1]].T.reshape(-1,2)\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d point in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "# Main loop for extracting corners\n",
    "for calib_img_path in tqdm(calib_paths):\n",
    "    img = read_process_img(calib_img_path) * white_balance\n",
    "    # Linear to grayscale conversion with scaling\n",
    "    img_gray = img[...,2] * 0.2126 + img[...,1] * 0.7152 + img[...,0] * 0.0722\n",
    "#     img_gray = np.average(img, axis=-1)\n",
    "    img_gray /= np.max(img_gray)\n",
    "    img_gray_resized = cv2.resize(\n",
    "        img_gray, \n",
    "        dsize=(img.shape[1] // downsample_scale, img.shape[0] // downsample_scale), \n",
    "        interpolation=cv2.INTER_AREA)\n",
    "    ret, corners_resized = cv2.findChessboardCorners(\n",
    "        (img_gray_resized * 255).astype(np.uint8), checkerboard_size, None)\n",
    "    # If found, add object points, image points (after refining them)\n",
    "    if ret == True:\n",
    "        corners = corners_resized * downsample_scale\n",
    "        objpoints.append(objp)\n",
    "        corners2 = cv2.cornerSubPix(img_gray, np.floor(corners), (40,40), (-1,-1), criteria)\n",
    "        imgpoints.append(corners2)\n",
    "        \n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_gray.shape[::-1], None, None)\n",
    "print(\"Reprojection error:\", ret)\n",
    "\n",
    "# fig = plt.figure(figsize=(8,6))\n",
    "# plt.imshow(img_gray)\n",
    "# plt.plot(corners2[:,0,0], corners2[:,0,1], \"ro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd2e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = img_gray.shape[:2]\n",
    "exr_resize_shape = (1152, 768)\n",
    "upsample_factor = int(np.max(np.ceil(img_gray.shape[-1::-1] / np.array(exr_resize_shape))))\n",
    "\n",
    "w_new, h_new = np.array(exr_resize_shape) * upsample_factor\n",
    "newcameramtx, roi = cv2.getOptimalNewCameraMatrix(\n",
    "    mtx, dist, (w,h), \n",
    "    0, # Contain only the valid pixels\n",
    "    (w_new,h_new)\n",
    ")\n",
    "mapx, mapy = cv2.initUndistortRectifyMap(mtx, dist, None, newcameramtx, (w_new,h_new), cv2.CV_32F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f87c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "exr_output_dir = root_dir / \"merged_images\"\n",
    "exr_output_dir.mkdir(exist_ok=True)\n",
    "png_output_dir = root_dir / \"png_images\"\n",
    "png_output_dir.mkdir(exist_ok=True)\n",
    "pose_output_dir = root_dir / \"pose_images\"\n",
    "pose_output_dir.mkdir(exist_ok=True)\n",
    "# Setup the image scales and output brightness\n",
    "pose_resize_scale = 4\n",
    "png_exposure_scale = 1.5\n",
    "depth_normal_exposure_scale = 2.0\n",
    "# Setup the params for combining images\n",
    "apply_median_blur = False\n",
    "times_min_index = np.argmin(times)\n",
    "times_max_index = np.argmax(times)\n",
    "exposure_target = times[-1]\n",
    "# Depth Normal estimation\n",
    "estimate_depth = True\n",
    "estimate_normals = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e00b2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "def process_bracket(index):\n",
    "# for index in tqdm(range(len(image_paths)//len(times))):\n",
    "#     index = 83\n",
    "    exr_path = exr_output_dir / f\"img_{str(index).zfill(4)}.exr\"\n",
    "    pose_path = pose_output_dir / f\"img_{str(index).zfill(4)}.png\"\n",
    "    png_path = png_output_dir / f\"img_{str(index).zfill(4)}.png\"\n",
    "    # Get the correct image index\n",
    "    index *= len(times)\n",
    "    \n",
    "    # Read and correct images\n",
    "    images = []\n",
    "    for i, time in enumerate(times):\n",
    "        images.append(read_process_img(str(image_paths[index])))\n",
    "        index += 1\n",
    "    images = np.array(images)\n",
    "    \n",
    "    # Calculate hat function for averaging the values\n",
    "    weights = np.minimum(images, 1.0 - images)\n",
    "    weights[times_max_index][images[times_max_index]<0.5] = 1.0\n",
    "    weights[times_min_index][images[times_min_index]>0.5] = 1.0\n",
    "    weights /= np.sum(weights, axis=0)\n",
    "    \n",
    "    # Scale the images to same exposure\n",
    "    images_scaled = images * (exposure_target / times)[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "    # Combine the images\n",
    "    imageHDR = np.sum(images_scaled * weights, axis=0) * white_balance\n",
    "    if apply_median_blur:\n",
    "        imageHDR = cv2.medianBlur(imageHDR, ksize=3)\n",
    "        \n",
    "    # Undistort hdr image\n",
    "    imageHDR = cv2.remap(imageHDR, mapx, mapy, cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Write exr image for reconstruction\n",
    "    output_img_resized = cv2.resize(\n",
    "        imageHDR, \n",
    "        dsize=exr_resize_shape, \n",
    "        interpolation=cv2.INTER_AREA)\n",
    "    cv2.imwrite(str(exr_path), output_img_resized)\n",
    "    png_image = np.clip(output_img_resized * png_exposure_scale, a_min=0, a_max=1) **(1/2.2)\n",
    "    png_image = (png_image * 255).astype(np.uint8)\n",
    "    cv2.imwrite(str(png_path), png_image)\n",
    "    \n",
    "    # Write png pose image for colmap poses\n",
    "    pose_image = cv2.resize(\n",
    "        imageHDR, \n",
    "        dsize=(exr_resize_shape[0]*pose_resize_scale, exr_resize_shape[1]*pose_resize_scale), \n",
    "        interpolation=cv2.INTER_AREA)\n",
    "    pose_image = np.clip(pose_image * png_exposure_scale, a_min=0, a_max=1) **(1/2.2)\n",
    "    pose_image = (pose_image * 255).astype(np.uint8)\n",
    "    cv2.imwrite(str(pose_path), pose_image)\n",
    "    \n",
    "\n",
    "tic = time.time()\n",
    "p = Pool(processes=8)\n",
    "index_list = range(len(image_paths)//len(times))\n",
    "list(tqdm(p.imap_unordered(process_bracket, index_list), total=len(index_list)))\n",
    "p.close()\n",
    "p.join()\n",
    "print('==== ...DONE. Took %.2f seconds'%(time.time() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa566fec",
   "metadata": {},
   "source": [
    "## Colmap reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bcead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfstudio.process_data.colmap_utils import get_colmap_version, get_vocab_tree\n",
    "from nerfstudio.utils.rich_utils import status\n",
    "from nerfstudio.utils.scripts import run_command\n",
    "from rich.console import Console\n",
    "CONSOLE = Console(width=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ff61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CameraModel(Enum):\n",
    "    \"\"\"Enum for camera types.\"\"\"\n",
    "\n",
    "    OPENCV = \"OPENCV\"\n",
    "    OPENCV_FISHEYE = \"OPENCV_FISHEYE\"\n",
    "    PINHOLE = \"PINHOLE\"\n",
    "    SIMPLE_PINHOLE = \"SIMPLE_PINHOLE\"\n",
    "\n",
    "\n",
    "def run_colmap(\n",
    "    image_dir: Path,\n",
    "    colmap_dir: Path,\n",
    "    camera_model: CameraModel,\n",
    "    camera_mask_path: Optional[Path] = None,\n",
    "    camera_params = None,\n",
    "    gpu: bool = True,\n",
    "    verbose: bool = False,\n",
    "    matching_method: Literal[\"vocab_tree\", \"exhaustive\", \"sequential\"] = \"vocab_tree\",\n",
    "    colmap_cmd: str = \"colmap\",\n",
    "):\n",
    "    camera_params = '\"' + \",\".join([i.astype(str) for i in camera_params]) + '\"'\n",
    "    colmap_version = get_colmap_version(colmap_cmd)\n",
    "\n",
    "    colmap_database_path = colmap_dir / \"database.db\"\n",
    "    if colmap_database_path.exists():\n",
    "        # Can't use missing_ok argument because of Python 3.7 compatibility.\n",
    "        colmap_database_path.unlink()\n",
    "\n",
    "    # Feature extraction\n",
    "    feature_extractor_cmd = [\n",
    "        f\"{colmap_cmd} feature_extractor\",\n",
    "        f\"--database_path {colmap_dir / 'database.db'}\",\n",
    "        f\"--image_path {image_dir}\",\n",
    "        \"--ImageReader.single_camera 1\",\n",
    "        f\"--ImageReader.camera_model {camera_model.value}\",\n",
    "        f\"--ImageReader.camera_params {camera_params}\",\n",
    "        f\"--SiftExtraction.use_gpu {int(gpu)}\",\n",
    "    ]\n",
    "    if camera_mask_path is not None:\n",
    "        feature_extractor_cmd.append(f\"--ImageReader.camera_mask_path {camera_mask_path}\")\n",
    "    feature_extractor_cmd = \" \".join(feature_extractor_cmd)\n",
    "    with status(msg=\"[bold yellow]Running COLMAP feature extractor...\", spinner=\"moon\", verbose=verbose):\n",
    "        run_command(feature_extractor_cmd, verbose=verbose)\n",
    "\n",
    "    CONSOLE.log(\"[bold green]:tada: Done extracting COLMAP features.\")\n",
    "\n",
    "    # Feature matching\n",
    "    feature_matcher_cmd = [\n",
    "        f\"{colmap_cmd} {matching_method}_matcher\",\n",
    "        f\"--database_path {colmap_dir / 'database.db'}\",\n",
    "        f\"--SiftMatching.use_gpu {int(gpu)}\",\n",
    "    ]\n",
    "    if matching_method == \"vocab_tree\":\n",
    "        vocab_tree_filename = get_vocab_tree()\n",
    "        feature_matcher_cmd.append(f\"--VocabTreeMatching.vocab_tree_path {vocab_tree_filename}\")\n",
    "    feature_matcher_cmd = \" \".join(feature_matcher_cmd)\n",
    "    with status(msg=\"[bold yellow]Running COLMAP feature matcher...\", spinner=\"runner\", verbose=verbose):\n",
    "        run_command(feature_matcher_cmd, verbose=verbose)\n",
    "    CONSOLE.log(\"[bold green]:tada: Done matching COLMAP features.\")\n",
    "\n",
    "    # Bundle adjustment\n",
    "    sparse_dir = colmap_dir / \"sparse\"\n",
    "    sparse_dir.mkdir(parents=True, exist_ok=True)\n",
    "    mapper_cmd = [\n",
    "        f\"{colmap_cmd} mapper\",\n",
    "        f\"--database_path {colmap_dir / 'database.db'}\",\n",
    "        f\"--image_path {image_dir}\",\n",
    "        f\"--output_path {sparse_dir}\",\n",
    "        \"--Mapper.ba_refine_focal_length 0\",\n",
    "        \"--Mapper.ba_refine_extra_params 0\",\n",
    "    ]\n",
    "    if colmap_version >= 3.7:\n",
    "        mapper_cmd.append(\"--Mapper.ba_global_function_tolerance 1e-6\")\n",
    "\n",
    "    mapper_cmd = \" \".join(mapper_cmd)\n",
    "\n",
    "    with status(\n",
    "        msg=\"[bold yellow]Running COLMAP bundle adjustment... (This may take a while)\",\n",
    "        spinner=\"circle\",\n",
    "        verbose=verbose,\n",
    "    ):\n",
    "        run_command(mapper_cmd, verbose=verbose)\n",
    "    CONSOLE.log(\"[bold green]:tada: Done COLMAP bundle adjustment.\")\n",
    "    with status(msg=\"[bold yellow]Refine intrinsics...\", spinner=\"dqpb\", verbose=verbose):\n",
    "        bundle_adjuster_cmd = [\n",
    "            f\"{colmap_cmd} bundle_adjuster\",\n",
    "            f\"--input_path {sparse_dir}/0\",\n",
    "            f\"--output_path {sparse_dir}/0\",\n",
    "            \"--BundleAdjustment.refine_principal_point 0\",\n",
    "            \"--BundleAdjustment.refine_focal_length 0\",\n",
    "            \"--BundleAdjustment.refine_extra_params 0\",\n",
    "        ]\n",
    "        run_command(\" \".join(bundle_adjuster_cmd), verbose=verbose)\n",
    "    CONSOLE.log(\"[bold green]:tada: Done refining intrinsics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e3022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hloc import (\n",
    "    extract_features,\n",
    "    match_features,\n",
    "    pairs_from_exhaustive,\n",
    "    pairs_from_retrieval,\n",
    "    reconstruction,\n",
    ")\n",
    "import pycolmap\n",
    "\n",
    "def run_hloc(\n",
    "    image_dir: Path,\n",
    "    colmap_dir: Path,\n",
    "    camera_model: CameraModel,\n",
    "    camera_params = None,\n",
    "    verbose: bool = False,\n",
    "    matching_method: Literal[\"vocab_tree\", \"exhaustive\", \"sequential\"] = \"vocab_tree\",\n",
    "    feature_type: Literal[\n",
    "        \"sift\", \"superpoint_aachen\", \"superpoint_max\", \"superpoint_inloc\", \"r2d2\", \"d2net-ss\", \"sosnet\", \"disk\"\n",
    "    ] = \"superpoint_aachen\",\n",
    "    matcher_type: Literal[\n",
    "        \"superglue\", \"superglue-fast\", \"NN-superpoint\", \"NN-ratio\", \"NN-mutual\", \"adalam\"\n",
    "    ] = \"superglue\",\n",
    "    num_matched: int = 50,\n",
    ") -> None:\n",
    "    camera_params = \",\".join([i.astype(str) for i in camera_params])\n",
    "    \n",
    "    outputs = colmap_dir\n",
    "    sfm_pairs = outputs / \"pairs-netvlad.txt\"\n",
    "    sfm_dir = outputs / \"sparse\" / \"0\"\n",
    "    features = outputs / \"features.h5\"\n",
    "    matches = outputs / \"matches.h5\"\n",
    "\n",
    "    retrieval_conf = extract_features.confs[\"netvlad\"]\n",
    "    feature_conf = extract_features.confs[feature_type]\n",
    "    matcher_conf = match_features.confs[matcher_type]\n",
    "\n",
    "    references = [p.relative_to(image_dir).as_posix() for p in image_dir.iterdir()]\n",
    "    extract_features.main(feature_conf, image_dir, image_list=references, feature_path=features)\n",
    "    if matching_method == \"exhaustive\":\n",
    "        pairs_from_exhaustive.main(sfm_pairs, image_list=references)\n",
    "    else:\n",
    "        retrieval_path = extract_features.main(retrieval_conf, image_dir, outputs)\n",
    "        if num_matched >= len(references):\n",
    "            num_matched = len(references)\n",
    "        pairs_from_retrieval.main(retrieval_path, sfm_pairs, num_matched=num_matched)\n",
    "    match_features.main(matcher_conf, sfm_pairs, features=features, matches=matches)\n",
    "\n",
    "    image_options = pycolmap.ImageReaderOptions(  # pylint: disable=c-extension-no-member\n",
    "        camera_model=camera_model.value,\n",
    "        camera_params=camera_params\n",
    "    )\n",
    "    min_match_score = None\n",
    "    skip_geometric_verification = False\n",
    "    \n",
    "    sfm_dir.mkdir(parents=True, exist_ok=True)\n",
    "    database = sfm_dir / 'database.db'\n",
    "    \n",
    "    reconstruction.create_empty_db(database)\n",
    "    reconstruction.import_images(\n",
    "        image_dir, database, pycolmap.CameraMode.SINGLE, references, image_options)\n",
    "    image_ids = reconstruction.get_image_ids(database)\n",
    "    reconstruction.import_features(image_ids, database, features)\n",
    "    reconstruction.import_matches(image_ids, database, sfm_pairs, matches,\n",
    "                   min_match_score, skip_geometric_verification)\n",
    "    \n",
    "    mapper_options = {\n",
    "      \"ba_refine_focal_length\": False,\n",
    "      \"ba_refine_principal_point\": False,\n",
    "      \"ba_refine_extra_params\": False,\n",
    "    }\n",
    "    \n",
    "    if not skip_geometric_verification:\n",
    "        reconstruction.estimation_and_geometric_verification(\n",
    "            database, sfm_pairs, verbose)\n",
    "    rec_out = reconstruction.run_reconstruction(\n",
    "        sfm_dir, database, image_dir, verbose, mapper_options)\n",
    "    if rec_out is not None:\n",
    "        CONSOLE.log(f'Reconstruction statistics:\\n{rec_out.summary()}'\n",
    "                    + f'\\n\\tnum_input_images = {len(image_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfbcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "colmap_dir = root_dir / \"reconstructions\" / \"reconstruction_colmap\"\n",
    "# colmap_dir = root_dir / \"reconstructions\" / \"reconstruction_hloc\"\n",
    "recon_dir = colmap_dir / \"sparse\" / \"0\"\n",
    "if colmap_dir.exists():\n",
    "    shutil.rmtree(colmap_dir)\n",
    "os.makedirs(colmap_dir)\n",
    "\n",
    "camera_params = np.array(\n",
    "    [newcameramtx[0,0], newcameramtx[1,1], newcameramtx[0, 2], newcameramtx[1, 2]])\n",
    "camera_params *= pose_resize_scale / upsample_factor\n",
    "\n",
    "camera_model = CameraModel.PINHOLE\n",
    "\n",
    "if METHOD.lower() == 'colmap':\n",
    "    # USE COLMAP\n",
    "    run_colmap(\n",
    "        image_dir=pose_output_dir,\n",
    "        colmap_dir=colmap_dir,\n",
    "        camera_model=camera_model,\n",
    "        camera_params=camera_params,\n",
    "        gpu=True,\n",
    "        verbose=True,\n",
    "        matching_method=\"exhaustive\"\n",
    "    ); appendix = '_colmap'\n",
    "\n",
    "elif METHOD.lower() == 'superglue':\n",
    "    # USE Superglue\n",
    "    run_hloc(\n",
    "        image_dir=pose_output_dir,\n",
    "        colmap_dir=colmap_dir,\n",
    "        camera_model=camera_model,\n",
    "        camera_params=camera_params,\n",
    "        verbose=True,\n",
    "        matching_method=\"exhaustive\",\n",
    "        feature_type=\"superpoint_inloc\",\n",
    "        matcher_type=\"superglue\",\n",
    "    ); appendix = '_supergloo'\n",
    "else:\n",
    "    raise ValueError(f\"Unknown method {METHOD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0692ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfstudio.utils.io import load_from_json, write_to_json\n",
    "from nerfstudio.data.utils.colmap_parsing_utils import (\n",
    "    read_cameras_binary, read_images_binary, qvec2rotmat, \n",
    ")\n",
    "    \n",
    "from nerfstudio.process_data.colmap_utils import parse_colmap_camera_params\n",
    "\n",
    "recon_dir = colmap_dir / \"sparse\" / \"0\"\n",
    "\n",
    "cam_id_to_camera = read_cameras_binary(recon_dir / \"cameras.bin\")\n",
    "im_id_to_image = read_images_binary(recon_dir / \"images.bin\")\n",
    "\n",
    "print(f\"{len(im_id_to_image)} frames matched..\")\n",
    "\n",
    "frames = []\n",
    "for im_id, im_data in im_id_to_image.items():\n",
    "    # NB: COLMAP uses Eigen / scalar-first quaternions\n",
    "    # * https://colmap.github.io/format.html\n",
    "    # * https://github.com/colmap/colmap/blob/bf3e19140f491c3042bfd85b7192ef7d249808ec/src/base/pose.cc#L75\n",
    "    # the `rotation_matrix()` handles that format for us.\n",
    "\n",
    "    # TODO(1480) BEGIN use pycolmap API\n",
    "    # rotation = im_data.rotation_matrix()\n",
    "    rotation = qvec2rotmat(im_data.qvec)\n",
    "\n",
    "    translation = im_data.tvec.reshape(3, 1)\n",
    "    w2c = np.concatenate([rotation, translation], 1)\n",
    "    w2c = np.concatenate([w2c, np.array([[0, 0, 0, 1]])], 0)\n",
    "    c2w = np.linalg.inv(w2c)\n",
    "    # Convert from COLMAP's camera coordinate system (OpenCV) to ours (OpenGL)\n",
    "    c2w[0:3, 1:3] *= -1\n",
    "    c2w = c2w[np.array([1, 0, 2, 3]), :]\n",
    "    c2w[2, :] *= -1\n",
    "\n",
    "    name = im_data.name.split(\".\")[0]+\".exr\"\n",
    "    name = Path(f\"./{exr_output_dir.name}/{name}\")\n",
    "    \n",
    "#     name = im_data.name.split(\".\")[0]+\".png\"\n",
    "#     name = Path(f\"./{png_output_dir.name}/{name}\")\n",
    "\n",
    "    frame = {\n",
    "        \"file_path\": name.as_posix(),\n",
    "        \"transform_matrix\": c2w.tolist(),\n",
    "#         \"colmap_im_id\": im_id,\n",
    "    }\n",
    "    frames.append(frame)\n",
    "    \n",
    "# Read camera params and write frame information\n",
    "transforms_json = parse_colmap_camera_params(cam_id_to_camera[1])\n",
    "transforms_json[\"frames\"] = frames\n",
    "\n",
    "# Scale camera params\n",
    "cam_params = np.array([\n",
    "    transforms_json[\"w\"], transforms_json[\"h\"], \n",
    "    transforms_json[\"fl_x\"], transforms_json[\"fl_y\"], transforms_json[\"cx\"], transforms_json[\"cy\"]])\n",
    "cam_param_scaled = cam_params / pose_resize_scale\n",
    "\n",
    "transforms_json[\"w\"], transforms_json[\"h\"], \\\n",
    "    transforms_json[\"fl_x\"], transforms_json[\"fl_y\"],\\\n",
    "    transforms_json[\"cx\"], transforms_json[\"cy\"] = tuple(cam_param_scaled)\n",
    "\n",
    "write_to_json(root_dir/(\"transforms%s.json\"%appendix), transforms_json)\n",
    "print(f\"Wrote info to {root_dir/('transforms%s.json'%appendix)} file!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
